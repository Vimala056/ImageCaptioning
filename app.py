# Install required packages first (only needed once in Colab or local)
# !pip install streamlit transformers torch torchvision pillow requests

import streamlit as st
from transformers import BlipProcessor, BlipForConditionalGeneration
import requests
from PIL import Image
from io import BytesIO
import torch

# Load BLIP model and processor
@st.cache_resource
def load_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

processor, model = load_model()

# Streamlit UI
st.title("üñºÔ∏è Image Caption Generator using BLIP")
st.write("Enter an image URL and get a caption generated by a pre-trained BLIP model.")

img_url = st.text_input("Enter Image URL")

if img_url:
    try:
        response = requests.get(img_url)
        img = Image.open(BytesIO(response.content)).convert("RGB")
        st.image(img, caption="Input Image", use_column_width=True)

        with st.spinner("Generating caption..."):
            inputs = processor(images=img, return_tensors="pt")
            out = model.generate(**inputs)
            caption = processor.decode(out[0], skip_special_tokens=True)

        st.success("Caption Generated:")
        st.markdown(f"**{caption}**")

    except Exception as e:
        st.error(f"Failed to process image: {e}")
